{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MichalKucko/NLP/blob/master/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5ouvU9EwFE_",
        "colab_type": "code",
        "outputId": "50b7c5d3-b900-4575-99c6-634a5b4d644c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.stats.stats import pearsonr  \n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, LSTM, Embedding, Bidirectional, SimpleRNN, GRU\n",
        "from gensim.models import FastText\n",
        "from gensim.utils import tokenize\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn import svm\n",
        "import matplotlib.pyplot as plt\n",
        "import regex as re\n",
        "from google.colab import files"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4bGyOQira-S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# załadowanie plików z danymi\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCO8ULk_8cPF",
        "colab_type": "code",
        "outputId": "dbffed2a-3929-49f5-c37e-f1d012902a54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "# ściągnięcie i rozpakowanie wektorów FastText\n",
        "!curl -o cc.pl.300.bin.gz https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.pl.300.bin.gz\n",
        "!gunzip cc.pl.300.bin.gz"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 4294M  100 4294M    0     0  38.8M      0  0:01:50  0:01:50 --:--:-- 19.9M\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ss2i4TrzBjRX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# parsowanie za pomocą UDPipe\n",
        "! wget https://github.com/ufal/udpipe/releases/download/v1.2.0/udpipe-1.2.0-bin.zip\n",
        "! unzip udpipe-1.2.0-bin.zip\n",
        "! rm udpipe-1.2.0-bin.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rw1iynunBoD-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ['PATH'] = os.environ['PATH'] + ':udpipe-1.2.0-bin/bin-linux64/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGYTmrWCBq7r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! wget http://mozart.ipipan.waw.pl/~alina/Polish_dependency_parsing_models/190423_PDBUD_ttp_embedd.udpipe"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYQiQhv_BtGt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! udpipe --tokenize --outfile=train_tokenised.conllu 190423_PDBUD_ttp_embedd.udpipe training_set_clean_only_text.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4TaUxfHBz-s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! udpipe --tag --parse --outfile=train_udpipe_parsed.conllu 190423_PDBUD_ttp_embedd.udpipe train_tokenised.conllu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ib9CNCtFB1bS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! udpipe --tokenize --outfile=test_tokenised.conllu 190423_PDBUD_ttp_embedd.udpipe test_set_clean_only_text.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkRobIvkB2sl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! udpipe --tag --parse --outfile=test_udpipe_parsed.conllu 190423_PDBUD_ttp_embedd.udpipe test_tokenised.conllu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTMH-ejYmcE_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# wczytanie danych\n",
        "with open('training_set_clean_only_text.txt', 'r') as f:\n",
        "#with open('train_stems.txt', 'r') as f:\n",
        "  train_text = f.readlines()\n",
        "with open('test_set_clean_only_text.txt', 'r') as f:\n",
        "#with open('test_stems.txt', 'r') as f:\n",
        "  test_text = f.readlines()\n",
        "#train_tags = np.loadtxt('training_set_clean_only_tags.txt', dtype=int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Db4UoRfge6Kd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# wczytanie wyników parsowania\n",
        "with open('train_udpipe_parsed.conllu', 'r') as f:\n",
        "  train_parsed = f.readlines()\n",
        "with open('test_udpipe_parsed.conllu', 'r') as f:\n",
        "  test_parsed = f.readlines()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbpPwr4jqawq",
        "colab_type": "code",
        "outputId": "e2c6f87c-39cd-4547-9085-90430ab77cea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# baseline - losowanie klasy na podstawie ich częstości w zbiorze uczącym\n",
        "freq0 = len(train_tags[train_tags==0])/len(train_tags)\n",
        "print('Odsetek próbek klasy 0:', freq0)\n",
        "out = ['0\\n' if np.random.uniform() < freq0 else '1\\n' for i in range(len(test_text))]\n",
        "with open('resultsBaseline.txt', 'w') as f:\n",
        "  f.writelines(out)\n",
        "files.download('resultsBaseline.txt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Odsetek próbek klasy 0: 0.915247485310228\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBsKDe2kwDQi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# wczytanie modelu wektorów FastText\n",
        "vecModel = FastText.load_fasttext_format('cc.pl.300')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qNRfjjlfJPt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#funkcja lematyzująca na podstawie banku drzew\n",
        "\n",
        "lemmas={}\n",
        "\n",
        "for el in train_parsed:\n",
        "  if len(el.split(\"\\t\"))>=3:\n",
        "    lemmas[el.split(\"\\t\")[1]]=el.split(\"\\t\")[2]\n",
        "for el in test_parsed:\n",
        "  if len(el.split(\"\\t\"))>=3:\n",
        "    lemmas[el.split(\"\\t\")[1]]=el.split(\"\\t\")[2]\n",
        "\n",
        "def lemma(w):\n",
        "  if lemmas[w]=='_':\n",
        "    if w[-2:] in [\"am\", \"aś\",'em','eś']:\n",
        "      third_pers=w[:-2]\n",
        "    elif w[-1] in ['m','ś']:\n",
        "      third_pers=w[:-1]\n",
        "    elif w[-3:]=='śmy':\n",
        "      third_pers=w[:-3]\n",
        "    elif w[-4:]=='ście':\n",
        "      third_pers=w[:-3]\n",
        "    else:\n",
        "      third_pers=w\n",
        "    return lemmas[third_pers]\n",
        "  return lemmas[w]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgUWpkspBPgq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dla każdego przykładu zwraca, ile było słów spoza słownika\n",
        "def getMistakeCnts(text):\n",
        "  cntVec = np.zeros(len(text))\n",
        "  for i in range(len(text)):\n",
        "    #tokens = list(tokenize(text[i], lowercase = True)) \n",
        "    tokens = text_to_word_sequence(text[i], filters='!\"#$%&()*+,-;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
        "    cnt = 0\n",
        "    for token in tokens:\n",
        "      if token == 'anonymized_account' or re.search('http', token):\n",
        "        continue\n",
        "      if (token not in vecModel.wv.vocab):\n",
        "        cnt += 1\n",
        "    cntVec[i] = cnt\n",
        "  return cntVec\n",
        "  \n",
        "mistakesVec_train = getMistakeCnts(train_text)\n",
        "print(sum(mistakesVec_train != 0), '/', len(train_text), 'zdań z błędami na zbiorze uczącym')\n",
        "print(pearsonr(mistakesVec_train, train_tags))   # korelacja raczej nieduża\n",
        "mistakesVec_test = getMistakeCnts(test_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzzeIFLtZDDZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# zamiana zdań na wektory (średnia z wektorów słów)\n",
        "def text2Vectors(text, embeddingDim = 300, lemmatize = False, addMistakesnt = False):\n",
        "  vecLen = embeddingDim + 1 if addMistakesnt else embeddingDim\n",
        "  sentVecs = np.zeros((len(text), vecLen))\n",
        "  for i in range(len(text)):\n",
        "    tokens = list(tokenize(text[i], lowercase = True))   # usuwa liczby i interpunkcję\n",
        "    #tokens = text_to_word_sequence(text[i], filters='!\"#$%&()*+,-;<=>?@[\\\\]^_`{|}~\\t\\n')   # usuwa znaki z filters\n",
        "    vec = np.zeros(embeddingDim)\n",
        "    cnt = 0\n",
        "    misCnt = 0\n",
        "    for token in tokens:\n",
        "      if token == 'anonymized_account' or re.search('http', token):  \n",
        "        continue\n",
        "      if (addMistakesnt and token not in vecModel.wv.vocab):\n",
        "        misCnt += 1\n",
        "      try:\n",
        "        if lemmatize: token = lemma(token)\n",
        "        vec += vecModel.wv[lemma(token)]\n",
        "        cnt += 1\n",
        "      except KeyError:\n",
        "        continue\n",
        "    sentVec = vec / cnt if cnt else np.zeros(embeddingDim)\n",
        "    sentVecs[i,] = np.append(sentVec, misCnt) if addMistakesnt else sentVec\n",
        "  return sentVecs\n",
        "\n",
        "sentVecsTrain = text2Vectors(train_text, lemmatize = False, addMistakesnt = False)\n",
        "sentVecsTest = text2Vectors(test_text, lemmatize = False, addMistakesnt = False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cutStIP-DdE9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# PCA na wektorach\n",
        "newVecLen = 50\n",
        "pcaModel = PCA(n_components=newVecLen)\n",
        "pcaSentVecsTrain = pcaModel.fit_transform(sentVecsTrain)\n",
        "pcaSentVecsTest = pcaModel.transform(sentVecsTest)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Xc6W3Z09gTU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# bag of words\n",
        "vectorizer = CountVectorizer()\n",
        "bags_train = vectorizer.fit_transform(train_text)\n",
        "bags_test = vectorizer.transform(test_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9L-V0P_D82L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# SVM na wektorach albo bags of wordsach\n",
        "svmModel = svm.SVC(class_weight = {0:0.1, 1:0.9}, C = 10000, gamma='auto')\n",
        "svmModel.fit(sentVecsTrain, train_tags)\n",
        "#svmModel.fit(pcaSentVecsTrain, train_tags)\n",
        "#svmModel.fit(bags_train, train_tags)\n",
        "preds = svmModel.predict(sentVecsTest)\n",
        "#preds = svmModel.predict(pcaSentVecsTest)\n",
        "#preds = svmModel.predict(bags_test)\n",
        "np.savetxt('resultsSVM.txt', preds, fmt='%d')\n",
        "files.download('resultsSVM.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-uocwjV8l5a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# SVM na liczbach błędów (to tak tylko, żeby zobaczyć)\n",
        "svmModel = svm.SVC(class_weight = {0:0.1, 1:0.9}, C = 1000, gamma='auto')\n",
        "svmModel.fit(mistakesVec_train.reshape(-1, 1), train_tags)\n",
        "preds = svmModel.predict(mistakesVec_test.reshape(-1, 1))\n",
        "np.savetxt('resultsSVM.txt', preds, fmt='%d')\n",
        "files.download('resultsSVM.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3bRZNvi47M3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_text_clean = []\n",
        "for line in train_text:\n",
        "  train_text_clean.append(re.sub('@anonymized_account|http\\S+\\s', '', line))\n",
        "test_text_clean = []\n",
        "for line in test_text:\n",
        "  test_text_clean.append(re.sub('@anonymized_account|http\\S+\\s', '', line))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwDg3IHWo101",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# nadpróbkowanie przykładów z mową nienawiści (do sprawdzenia zamiast albo razem z parametrem class_weight przy uczeniu modelu)\n",
        "train_hate_text = np.repeat(np.array(train_text_clean)[train_tags==1], 9)\n",
        "train_text_long = np.concatenate((np.array(train_text)[train_tags==0], train_hate_text))\n",
        "train_hate_tags = np.repeat(np.array(train_tags)[train_tags==1], 9)\n",
        "train_tags_long = np.concatenate((train_tags[train_tags==0], train_hate_tags))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DNt8Oxqn1Rq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# usuwa słowa występujące rzadziej niż count_thres\n",
        "def preprocess(text, count_thres):\n",
        "  tokenizer = Tokenizer(filters='!\"#$%&()*+,-;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
        "  tokenizer.fit_on_texts(text)\n",
        "  words_to_remove = ['anonymized', 'account']\n",
        "  words_to_remove.extend([w for w,c in tokenizer.word_counts.items() if c < count_thres])\n",
        "  before_cnt = len(tokenizer.word_index)\n",
        "  for w in words_to_remove:\n",
        "    del tokenizer.word_index[w]\n",
        "    del tokenizer.word_docs[w]\n",
        "    del tokenizer.word_counts[w]\n",
        "  text_seqs = tokenizer.texts_to_sequences(train_text)\n",
        "  return (tokenizer, text_seqs, before_cnt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJOG4E5dECW3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lemmatizeText(text):\n",
        "  text_lemma = []\n",
        "  for i in range(len(text)):\n",
        "    tokens = list(tokenize(text[i], lowercase = True))   # usuwa liczby i interpunkcję\n",
        "    #tokens = text_to_word_sequence(text[i], filters='!\"#$%&()*+,-;<=>?@[\\\\]^_`{|}~\\t\\n')   # usuwa znaki z filters\n",
        "    line = []\n",
        "    for token in tokens:\n",
        "      if token == 'anonymized_account' or re.search('http', token):  \n",
        "        continue\n",
        "      try:\n",
        "        line.append(lemma(token))\n",
        "      except KeyError:\n",
        "        line.append(token)\n",
        "    text_lemma.append(' '.join(line))\n",
        "  return text_lemma\n",
        "\n",
        "train_text_lemmas = lemmatizeText(train_text)\n",
        "test_text_lemmas = lemmatizeText(test_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qdvSjrfBVB7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sieć LSTM, która sama będzie uczyć się wektorów słów\n",
        "\n",
        "maxLen = 40   # maksymalna długość zdania\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_text_lemmas)\n",
        "train_seqs = tokenizer.texts_to_sequences(train_text_lemmas)\n",
        "#tokenizer, train_seqs, before_cnt = preprocess(train_text, 5)\n",
        "#maxLen = max([len(x) for x in train_seqs])\n",
        "train_seqs = pad_sequences(train_seqs, padding='post', maxlen=maxLen)\n",
        "\n",
        "embeddingDim = 100   # długość wektorów\n",
        "lstmModel = Sequential()\n",
        "lstmModel.add(Embedding(len(tokenizer.word_index) + 1, embeddingDim, input_length=maxLen))\n",
        "#lstmModel.add(LSTM(embeddingDim, unroll=True))\n",
        "lstmModel.add(Bidirectional(LSTM(embeddingDim, unroll=True)))\n",
        "#lstmModel.add(SimpleRNN(embeddingDim, unroll=True))\n",
        "#lstmModel.add(GRU(embeddingDim, unroll=True))\n",
        "#lstmModel.add(Dropout(0.4)) \n",
        "lstmModel.add(Dense(1, activation='sigmoid'))\n",
        "lstmModel.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "lstmModel.summary()\n",
        "history = lstmModel.fit(train_seqs, train_tags, epochs=10, class_weight = {0:0.1, 1:0.9})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSXqolVV_Ql8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sieć LSTM z wyuczonymi wektorami słów (FastText)\n",
        "\n",
        "maxLen = 40   # maksymalna długość zdania\n",
        "embeddingDim = 300   # długość wektorów (taka jest w FastText)\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_text_clean)\n",
        "train_seqs = tokenizer.texts_to_sequences(train_text_clean)\n",
        "#maxLen = max([len(x) for x in train_seqs])\n",
        "train_seqs = pad_sequences(train_seqs, padding='post', maxlen=maxLen)\n",
        "\n",
        "embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, embeddingDim))\n",
        "for word, index in tokenizer.word_index.items():\n",
        "  try:\n",
        "    vector = vecModel.wv[word]\n",
        "    embedding_matrix[index] = vector\n",
        "  except KeyError:\n",
        "    continue\n",
        "\n",
        "newEmbeddingDim = 100    # długość wektorów po PCA\n",
        "pcaModel = PCA(n_components=newEmbeddingDim)\n",
        "embedding_matrix_pca = pcaModel.fit_transform(embedding_matrix)\n",
        "#embedding_matrix_pca = embedding_matrix\n",
        " \n",
        "lstmModel = Sequential()\n",
        "lstmModel.add(Embedding(len(tokenizer.word_index) + 1, newEmbeddingDim, input_length=maxLen, weights=[embedding_matrix_pca], trainable=False))\n",
        "#lstmModel.add(LSTM(newEmbeddingDim))\n",
        "lstmModel.add(Bidirectional(LSTM(newEmbeddingDim, unroll=True)))\n",
        "#lstmModel.add(Dropout(0.5))\n",
        "lstmModel.add(Dense(1, activation='sigmoid'))\n",
        "lstmModel.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "lstmModel.summary()\n",
        "history = lstmModel.fit(train_seqs, train_tags, epochs=10, class_weight = {0:0.1, 1:0.9})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaBehaTiAuLM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# testowanie sieci (nowe słowa są odrzucane)\n",
        "test_seqs = tokenizer.texts_to_sequences(test_text_lemmas)\n",
        "test_seqs = pad_sequences(test_seqs, padding='post', maxlen=maxLen)\n",
        "preds = lstmModel.predict(test_seqs)\n",
        "preds[preds > 0.5] = 1\n",
        "preds[preds <= 0.5] = 0\n",
        "np.savetxt('resultsLSTM.txt', preds, fmt='%d')\n",
        "files.download('resultsLSTM.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkXBgEQWmNig",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sieć LSTM na wektorach FastText (bez warstwy Embedding)\n",
        "\n",
        "#maxLen = 40   # maksymalna długość zdania\n",
        "embeddingDim = 300   # długość wektorów (taka jest w FastText)\n",
        "#tokenizer, train_seqs, before_cnt = preprocess(train_text, 3)\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_text_clean)\n",
        "train_seqs = tokenizer.texts_to_sequences(train_text_clean)\n",
        "maxLen = max([len(x) for x in train_seqs])\n",
        "train_seqs = pad_sequences(train_seqs, padding='post', maxlen=maxLen)\n",
        "\n",
        "embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, embeddingDim))\n",
        "for word, index in tokenizer.word_index.items():\n",
        "  try:\n",
        "    vector = vecModel.wv[word]\n",
        "    embedding_matrix[index] = vector\n",
        "  except KeyError:\n",
        "    continue\n",
        "\n",
        "newEmbeddingDim = 100    # długość wektorów po PCA\n",
        "#pcaModel = PCA(n_components=newEmbeddingDim)\n",
        "#embedding_matrix_pca = pcaModel.fit_transform(embedding_matrix)\n",
        "embedding_matrix_pca=embedding_matrix\n",
        "\n",
        "train_seqs = np.array([embedding_matrix_pca[seq,] for seq in train_seqs])\n",
        "\n",
        "lstmModel = Sequential()\n",
        "#lstmModel.add(LSTM(newEmbeddingDim))\n",
        "lstmModel.add(Bidirectional(LSTM(newEmbeddingDim, unroll=True, return_sequences=True)))\n",
        "lstmModel.add(Bidirectional(LSTM(50, unroll=True)))\n",
        "#lstmModel.add(Dropout(0.45))\n",
        "lstmModel.add(Dense(1, activation='sigmoid'))\n",
        "lstmModel.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "#lstmModel.summary()\n",
        "history = lstmModel.fit(train_seqs, train_tags, epochs=10, class_weight = {0:0.1, 1:0.9})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0YqPplTtap2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# testowanie sieci\n",
        "\n",
        "#tokenizer, test_seqs, before_cnt = preprocess(test_text, 5)\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(test_text_clean)\n",
        "test_seqs = tokenizer.texts_to_sequences(test_text_clean)\n",
        "maxLen = max([len(x) for x in train_seqs])\n",
        "test_seqs = pad_sequences(test_seqs, padding='post', maxlen=maxLen)\n",
        "\n",
        "embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, embeddingDim))\n",
        "for word, index in tokenizer.word_index.items():\n",
        "  try:\n",
        "    vector = vecModel.wv[word]\n",
        "    embedding_matrix[index] = vector\n",
        "  except KeyError:\n",
        "    continue\n",
        "\n",
        "embedding_matrix_pca = embedding_matrix\n",
        "#embedding_matrix_pca = pcaModel.transform(embedding_matrix)\n",
        "test_seqs = np.array([embedding_matrix_pca[seq,] for seq in test_seqs])\n",
        "\n",
        "preds = lstmModel.predict(test_seqs)\n",
        "preds[preds > 0.5] = 1\n",
        "preds[preds <= 0.5] = 0\n",
        "np.savetxt('resultsLSTM.txt', preds, fmt='%d')\n",
        "files.download('resultsLSTM.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R49Wndw2DAJ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "slownik = pd.read_csv(\"slownikWydzwieku01.csv\",sep=\"\\t\",header=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aotSBWhdDBnn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wydzwiek=slownik.iloc[:,[0,3]]\n",
        "wydzwiek.columns=[\"słowo\",\"ocena\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKjvg8YQDEKT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sentiment(text):\n",
        "  sentVecs = np.zeros((len(text), vecLen))\n",
        "  for i in range(len(text)):\n",
        "    tokens = list(tokenize(text[i], lowercase = True))   # usuwa liczby i interpunkcję\n",
        "    vec = np.zeros(vecLen)\n",
        "    cnt = 0\n",
        "    for token in tokens:\n",
        "      if token == 'anonymized_account':  \n",
        "        continue\n",
        "      try:\n",
        "        vec += vecModel.wv[token]\n",
        "        cnt += 1\n",
        "      except KeyError:\n",
        "        continue\n",
        "    sentVecs[i,] = vec / cnt if cnt else np.zeros(vecLen)\n",
        "  return sentVecs"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}