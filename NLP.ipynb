{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of NLP.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MichalKucko/NLP/blob/master/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5ouvU9EwFE_",
        "colab_type": "code",
        "outputId": "78b7d041-9495-4c2b-ed83-6bd1edf7e9e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.stats.stats import pearsonr  \n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, LSTM, Embedding, Bidirectional\n",
        "from gensim.models import FastText\n",
        "from gensim.utils import tokenize\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn import svm\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4bGyOQira-S",
        "colab_type": "code",
        "outputId": "51b40deb-12c8-48e4-a2b7-60cc98ef2138",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "source": [
        "# załadowanie plików z danymi\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-020373f8-9fe5-446f-bc64-8906f082a504\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-020373f8-9fe5-446f-bc64-8906f082a504\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving test_set_clean_only_tags.txt to test_set_clean_only_tags.txt\n",
            "Saving test_set_clean_only_text.txt to test_set_clean_only_text.txt\n",
            "Saving training_set_clean_only_tags.txt to training_set_clean_only_tags.txt\n",
            "Saving training_set_clean_only_text.txt to training_set_clean_only_text.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCO8ULk_8cPF",
        "colab_type": "code",
        "outputId": "69e4904b-a95f-4347-f3b2-6899063b1d83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# ściągnięcie i rozpakowanie wektorów FastText\n",
        "!curl -o cc.pl.300.bin.gz https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.pl.300.bin.gz\n",
        "!gunzip cc.pl.300.bin.gz"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 4294M  100 4294M    0     0  29.8M      0  0:02:23  0:02:23 --:--:-- 30.1M\n",
            "cc.pl.300.bin  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTMH-ejYmcE_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# wczytanie danych\n",
        "with open('training_set_clean_only_text.txt', 'r') as f:\n",
        "  train_text = f.readlines()\n",
        "with open('test_set_clean_only_text.txt', 'r') as f:\n",
        "  test_text = f.readlines()\n",
        "train_tags = np.loadtxt('training_set_clean_only_tags.txt', dtype=int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbpPwr4jqawq",
        "colab_type": "code",
        "outputId": "f3487119-2c6e-4db5-8547-896123a6b184",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# baseline - losowanie klasy na podstawie ich częstości w zbiorze uczącym\n",
        "freq0 = len(train_tags[train_tags==0])/len(train_tags)\n",
        "print('Odsetek próbek klasy 0:', freq0)\n",
        "out = ['0\\n' if np.random.uniform() < freq0 else '1\\n' for i in range(len(test_text))]\n",
        "with open('resultsBaseline.txt', 'w') as f:\n",
        "  f.writelines(out)\n",
        "files.download('resultsBaseline.txt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Odsetek próbek klasy 0: 0.915247485310228\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBsKDe2kwDQi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# wczytanie modelu wektorów FastText\n",
        "vecModel = FastText.load_fasttext_format('cc.pl.300')\n",
        "print(\"zmieniłam\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzzeIFLtZDDZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# zamiana zdań na wektory (średnia z wektorów słów)\n",
        "def text2Vectors(text, vecLen = 300):\n",
        "  sentVecs = np.zeros((len(text), vecLen))\n",
        "  for i in range(len(text)):\n",
        "    tokens = list(tokenize(text[i], lowercase = True))   # usuwa liczby i interpunkcję\n",
        "    vec = np.zeros(vecLen)\n",
        "    cnt = 0\n",
        "    for token in tokens:\n",
        "      if token == 'anonymized_account':  \n",
        "        continue\n",
        "      try:\n",
        "        vec += vecModel.wv[token]\n",
        "        cnt += 1\n",
        "      except KeyError:\n",
        "        continue\n",
        "    sentVecs[i,] = vec / cnt if cnt else np.zeros(vecLen)\n",
        "  return sentVecs\n",
        "\n",
        "sentVecsTrain = text2Vectors(train_text)\n",
        "sentVecsTest = text2Vectors(test_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cutStIP-DdE9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# PCA na wektorach\n",
        "newVecLen = 50\n",
        "#tsneModel = TSNE(perplexity=40, n_components=ncomponents, init='pca', metric=metric, n_iter=2500, random_state=23)  # można zobaczyć, czy TSNe lepsze od PCA\n",
        "pcaModel = PCA(n_components=newVecLen)\n",
        "pcaSentVecsTrain = pcaModel.fit_transform(sentVecsTrain)\n",
        "pcaSentVecsTest = pcaModel.transform(sentVecsTest)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Xc6W3Z09gTU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# bag of words\n",
        "vectorizer = CountVectorizer()\n",
        "bags_train = vectorizer.fit_transform(train_text)\n",
        "bags_test = vectorizer.transform(test_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9L-V0P_D82L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# SVM na wektorach albo bags of wordsach\n",
        "svmModel = svm.SVC(class_weight = {0:0.1, 1:0.9}, C = 1000, gamma='auto')\n",
        "svmModel.fit(sentVecsTrain, train_tags)\n",
        "#svmModel.fit(pcaSentVecsTrain, train_tags)\n",
        "#svmModel.fit(bags_train, train_tags)\n",
        "preds = svmModel.predict(sentVecsTest)\n",
        "#preds = svmModel.predict(pcaSentVecsTest)\n",
        "#preds = svmModel.predict(bags_test)\n",
        "np.savetxt('resultsSVM.txt', preds, fmt='%d')\n",
        "files.download('resultsSVM.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a71Lk31wLJDY",
        "colab_type": "code",
        "outputId": "dcddb3b9-a178-4e0e-9ad1-8d1ce2a2a2c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# dla każdego przykładu zwraca, ile było słów spoza słownika\n",
        "def getMistakeCnts(text):\n",
        "  cntVec = np.zeros(len(text))\n",
        "  for i in range(len(text)):\n",
        "    tokens = list(tokenize(text[i], lowercase = True))   #wywala liczby\n",
        "    cnt = 0\n",
        "    for token in tokens:\n",
        "      if token == 'anonymized_account':\n",
        "        continue\n",
        "      if (token not in vecModel.wv.vocab):\n",
        "        cnt += 1\n",
        "    cntVec[i] = cnt\n",
        "  return cntVec\n",
        "  \n",
        "mistakesVec_train = getMistakeCnts(train_text)\n",
        "mistakesVec_test = getMistakeCnts(test_text)\n",
        "print(sum(mistakesVec_train != 0), '/', len(train_text), 'zdań z błędami na zbiorze uczącym')\n",
        "print(pearsonr(mistakesVec_train, train_tags))   # korelacji raczej nie ma"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2954 / 10041 zdań z błędami na zbiorze uczącym\n",
            "(0.01720474005221367, 0.08472403401794718)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_l7HQeOcN6TT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# SVM na liczbach błędów\n",
        "svmModel = svm.SVC(class_weight = {0:0.1, 1:0.9}, C = 1000, gamma='auto')\n",
        "svmModel.fit(mistakesVec_train.reshape(-1, 1), train_tags)\n",
        "preds = svmModel.predict(mistakesVec_test.reshape(-1, 1))\n",
        "np.savetxt('resultsSVM.txt', preds, fmt='%d')\n",
        "files.download('resultsSVM.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qdvSjrfBVB7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sieć LSTM, która sama będzie uczyć się wektorów słów\n",
        "\n",
        "maxLen = 40   # maksymalna długość zdania\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_text)\n",
        "train_seqs = tokenizer.texts_to_sequences(train_text)\n",
        "#maxLen = max([len(x) for x in train_seqs])\n",
        "train_seqs = pad_sequences(train_seqs, padding='post', maxlen=maxLen)\n",
        "\n",
        "embeddingDim = 50   # długość wektorów\n",
        "lstmModel = Sequential()\n",
        "lstmModel.add(Embedding(len(tokenizer.word_index) + 1, embeddingDim, input_length=maxLen))\n",
        "lstmModel.add(LSTM(embeddingDim))\n",
        "lstmModel.add(Dropout(0.5))   # można zobaczyć różne wartości\n",
        "lstmModel.add(Dense(1, activation='sigmoid'))\n",
        "lstmModel.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "lstmModel.summary()\n",
        "history = lstmModel.fit(train_seqs, train_tags, epochs=30)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSXqolVV_Ql8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sieć LSTM z wyuczonymi wektorami słów (FastText)\n",
        "\n",
        "maxLen = 40   # maksymalna długość zdania\n",
        "embeddingDim = 300   # długość wektorów (taka jest w FastText)\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_text)\n",
        "train_seqs = tokenizer.texts_to_sequences(train_text)\n",
        "#maxLen = max([len(x) for x in train_seqs])\n",
        "train_seqs = pad_sequences(train_seqs, padding='post', maxlen=maxLen)\n",
        "\n",
        "embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, embeddingDim))\n",
        "for word, index in tokenizer.word_index.items():\n",
        "  try:\n",
        "    vector = vecModel.wv[word]\n",
        "    embedding_matrix[index] = vector\n",
        "  except KeyError:\n",
        "    continue\n",
        "\n",
        "newEmbeddingDim = 50    # długość wektorów po PCA\n",
        "pcaModel = PCA(n_components=newEmbeddingDim)\n",
        "embedding_matrix_pca = pcaModel.fit_transform(embedding_matrix)\n",
        "    \n",
        "lstmModel = Sequential()\n",
        "lstmModel.add(Embedding(len(tokenizer.word_index) + 1, newEmbeddingDim, input_length=maxLen, weights=[embedding_matrix_pca], trainable=False))\n",
        "lstmModel.add(LSTM(newEmbeddingDim))\n",
        "lstmModel.add(Dropout(0.5))\n",
        "lstmModel.add(Dense(1, activation='sigmoid'))\n",
        "lstmModel.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "lstmModel.summary()\n",
        "history = lstmModel.fit(train_seqs, train_tags)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaBehaTiAuLM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# testowanie sieci (nowe słowa są odrzucane)\n",
        "test_seqs = tokenizer.texts_to_sequences(test_text)\n",
        "test_seqs = pad_sequences(test_seqs, padding='post', maxlen=maxLen)\n",
        "preds = lstmModel.predict(test_seqs)\n",
        "preds[preds > 0.5] = 1\n",
        "preds[preds <= 0.5] = 0\n",
        "np.savetxt('resultsLSTM.txt', preds, fmt='%d')\n",
        "files.download('resultsLSTM.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkXBgEQWmNig",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sieć LSTM na wektorach FastText (bez warstwy Embedding)\n",
        "\n",
        "maxLen = 40   # maksymalna długość zdania\n",
        "embeddingDim = 300   # długość wektorów (taka jest w FastText)\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_text)\n",
        "train_seqs = tokenizer.texts_to_sequences(train_text)\n",
        "#maxLen = max([len(x) for x in train_seqs])\n",
        "train_seqs = pad_sequences(train_seqs, padding='post', maxlen=maxLen)\n",
        "\n",
        "embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, embeddingDim))\n",
        "for word, index in tokenizer.word_index.items():\n",
        "  try:\n",
        "    vector = vecModel.wv[word]\n",
        "    embedding_matrix[index] = vector\n",
        "  except KeyError:\n",
        "    continue\n",
        "\n",
        "newEmbeddingDim = 50    # długość wektorów po PCA\n",
        "pcaModel = PCA(n_components=newEmbeddingDim)\n",
        "embedding_matrix_pca = pcaModel.fit_transform(embedding_matrix)\n",
        "\n",
        "train_seqs = np.array([embedding_matrix_pca[seq,] for seq in train_seqs])\n",
        "  \n",
        "lstmModel = Sequential()\n",
        "lstmModel.add(LSTM(newEmbeddingDim))\n",
        "#lstmModel.add(Bidirectional(LSTM(newEmbeddingDim)))\n",
        "lstmModel.add(Dropout(0.5))\n",
        "lstmModel.add(Dense(1, activation='sigmoid'))\n",
        "lstmModel.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "#lstmModel.summary()\n",
        "history = lstmModel.fit(train_seqs, train_tags, epochs=30)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0YqPplTtap2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# testowanie sieci\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(test_text)\n",
        "test_seqs = tokenizer.texts_to_sequences(test_text)\n",
        "test_seqs = pad_sequences(test_seqs, padding='post', maxlen=maxLen)\n",
        "\n",
        "embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, embeddingDim))\n",
        "for word, index in tokenizer.word_index.items():\n",
        "  try:\n",
        "    vector = vecModel.wv[word]\n",
        "    embedding_matrix[index] = vector\n",
        "  except KeyError:\n",
        "    continue\n",
        "    \n",
        "embedding_matrix_pca = pcaModel.transform(embedding_matrix)\n",
        "test_seqs = np.array([embedding_matrix_pca[seq,] for seq in test_seqs])\n",
        "\n",
        "preds = lstmModel.predict(test_seqs)\n",
        "preds[preds > 0.5] = 1\n",
        "preds[preds <= 0.5] = 0\n",
        "np.savetxt('resultsLSTM.txt', preds, fmt='%d')\n",
        "files.download('resultsLSTM.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "211cLFnpI4pU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        },
        "outputId": "372c9394-d259-41ba-8c25-f4635269d178"
      },
      "source": [
        "! wget https://github.com/ufal/udpipe/releases/download/v1.2.0/udpipe-1.2.0-bin.zip\n",
        "! unzip udpipe-1.2.0-bin.zip\n",
        "! rm udpipe-1.2.0-bin.zip"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-06-02 14:24:27--  https://github.com/ufal/udpipe/releases/download/v1.2.0/udpipe-1.2.0-bin.zip\n",
            "Resolving github.com (github.com)... 192.30.253.112\n",
            "Connecting to github.com (github.com)|192.30.253.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github-production-release-asset-2e65be.s3.amazonaws.com/50672597/a24cacd8-77c6-11e7-8f6e-e9de8ca37f48?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20190602%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20190602T142427Z&X-Amz-Expires=300&X-Amz-Signature=d1da91a8dfc84f93118a9b192770e980d8745bb916d18d2a2a4f249085b15919&X-Amz-SignedHeaders=host&actor_id=0&response-content-disposition=attachment%3B%20filename%3Dudpipe-1.2.0-bin.zip&response-content-type=application%2Foctet-stream [following]\n",
            "--2019-06-02 14:24:27--  https://github-production-release-asset-2e65be.s3.amazonaws.com/50672597/a24cacd8-77c6-11e7-8f6e-e9de8ca37f48?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20190602%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20190602T142427Z&X-Amz-Expires=300&X-Amz-Signature=d1da91a8dfc84f93118a9b192770e980d8745bb916d18d2a2a4f249085b15919&X-Amz-SignedHeaders=host&actor_id=0&response-content-disposition=attachment%3B%20filename%3Dudpipe-1.2.0-bin.zip&response-content-type=application%2Foctet-stream\n",
            "Resolving github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)... 52.216.106.204\n",
            "Connecting to github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)|52.216.106.204|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12644197 (12M) [application/octet-stream]\n",
            "Saving to: ‘udpipe-1.2.0-bin.zip’\n",
            "\n",
            "udpipe-1.2.0-bin.zi 100%[===================>]  12.06M  17.9MB/s    in 0.7s    \n",
            "\n",
            "2019-06-02 14:24:28 (17.9 MB/s) - ‘udpipe-1.2.0-bin.zip’ saved [12644197/12644197]\n",
            "\n",
            "Archive:  udpipe-1.2.0-bin.zip\n",
            "replace udpipe-1.2.0-bin/bin-linux32/csharp/Ufal/UDPipe/Trainer.cs? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exvgbUANKev_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ['PATH'] = os.environ['PATH'] + ':udpipe-1.2.0-bin/bin-linux64/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MNIe3URJu_4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "outputId": "f4e0f78a-5706-4840-9518-6b8bbc57abc6"
      },
      "source": [
        "! udpipe"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Usage: udpipe [running_opts] model_file [input_files]\n",
            "       udpipe --train [training_opts] model_file [input_files]\n",
            "       udpipe --detokenize [detokenize_opts] raw_text_file [input_files]\n",
            "Running opts: --accuracy (measure accuracy only)\n",
            "              --input=[conllu|generic_tokenizer|horizontal|vertical]\n",
            "              --immediate (process sentences immediately during loading)\n",
            "              --outfile=output file template\n",
            "              --output=[conllu|epe|matxin|horizontal|plaintext|vertical]\n",
            "              --tokenize (perform tokenization)\n",
            "              --tokenizer=tokenizer options, implies --tokenize\n",
            "              --tag (perform tagging)\n",
            "              --tagger=tagger options, implies --tag\n",
            "              --parse (perform parsing)\n",
            "              --parser=parser options, implies --parse\n",
            "Training opts: --method=[morphodita_parsito] which method to use\n",
            "               --heldout=heldout data file name\n",
            "               --tokenizer=tokenizer options\n",
            "               --tagger=tagger options\n",
            "               --parser=parser options\n",
            "Detokenize opts: --outfile=output file template\n",
            "Generic opts: --version\n",
            "              --help\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdPO1nRPLj0U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "f3542605-5012-4cba-f46a-903ea549c418"
      },
      "source": [
        "! wget http://mozart.ipipan.waw.pl/~alina/Polish_dependency_parsing_models/190423_PDBUD_ttp_embedd.udpipe"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-06-02 14:30:05--  http://mozart.ipipan.waw.pl/~alina/Polish_dependency_parsing_models/190423_PDBUD_ttp_embedd.udpipe\n",
            "Resolving mozart.ipipan.waw.pl (mozart.ipipan.waw.pl)... 213.135.36.148\n",
            "Connecting to mozart.ipipan.waw.pl (mozart.ipipan.waw.pl)|213.135.36.148|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 53636990 (51M)\n",
            "Saving to: ‘190423_PDBUD_ttp_embedd.udpipe.1’\n",
            "\n",
            "190423_PDBUD_ttp_em 100%[===================>]  51.15M  14.7MB/s    in 4.4s    \n",
            "\n",
            "2019-06-02 14:30:10 (11.6 MB/s) - ‘190423_PDBUD_ttp_embedd.udpipe.1’ saved [53636990/53636990]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9oOER0pKJMc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "581f2005-3e1d-43a1-b9a7-a48bba59d07e"
      },
      "source": [
        "! udpipe --tokenize --outfile=train_tokenised.conllu 190423_PDBUD_ttp_embedd.udpipe training_set_clean_only_text.txt"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading UDPipe model: done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KQxalp8P1Q0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "be4160dc-1236-4c12-edba-3e6ac13f30a2"
      },
      "source": [
        "! udpipe --tag --parse --outfile=train_udpipe_parsed.conllu 190423_PDBUD_ttp_embedd.udpipe train_tokenised.conllu"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading UDPipe model: done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxMgYmeVSoVl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c87c418a-4e38-4ee6-9671-766d6d8f0cc6"
      },
      "source": [
        "! udpipe --tokenize --outfile=test_tokenised.conllu 190423_PDBUD_ttp_embedd.udpipe test_set_clean_only_text.txt"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading UDPipe model: done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiiRZVcKTLPC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "af2a51b3-724d-424a-f526-2014680c86e1"
      },
      "source": [
        "! udpipe --tag --parse --outfile=test_udpipe_parsed.conllu 190423_PDBUD_ttp_embedd.udpipe test_tokenised.conllu"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading UDPipe model: done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgSVBmKoTT6Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('train_udpipe_parsed.conllu', 'r') as f:\n",
        "  train_parsed = f.readlines()\n",
        "with open('test_udpipe_parsed.conllu', 'r') as f:\n",
        "  test_parsed = f.readlines()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcoeNJ1mmjs5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " #funkcja lematyzująca na podstawie banku drzew\n",
        "\n",
        "lemmas={}\n",
        "\n",
        "for el in train_parsed:\n",
        "  if len(el.split(\"\\t\"))>=3:\n",
        "    lemmas[el.split(\"\\t\")[1]]=el.split(\"\\t\")[2]\n",
        "for el in test_parsed:\n",
        "  if len(el.split(\"\\t\"))>=3:\n",
        "    lemmas[el.split(\"\\t\")[1]]=el.split(\"\\t\")[2]\n",
        "\n",
        "def lemma(w):\n",
        "  if lemmas[w]=='_':\n",
        "    if w[-2:] in ['em','eś']:\n",
        "      third_pers=w[:-2]\n",
        "    elif w[-1] in ['m','ś']:\n",
        "      third_pers=w[:-1]\n",
        "    elif w[-3:]=='śmy':\n",
        "      third_pers=w[:-3]\n",
        "    elif w[-4:]=='ście':\n",
        "      third_pers=w[:-3]\n",
        "    else:\n",
        "      third_pers=w\n",
        "    return lemmas[third_pers]\n",
        "  return lemmas[w]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3VzeGHnLXsr",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}