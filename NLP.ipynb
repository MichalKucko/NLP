{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MichalKucko/NLP/blob/master/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5ouvU9EwFE_",
        "colab_type": "code",
        "outputId": "61ee1dcf-e578-45ca-eca6-cc4caaaef793",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.stats.stats import pearsonr  \n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, LSTM, Embedding, Bidirectional, SimpleRNN, GRU\n",
        "from gensim.models import FastText\n",
        "from gensim.utils import tokenize\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn import svm\n",
        "import matplotlib.pyplot as plt\n",
        "import regex as re\n",
        "from google.colab import files"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCO8ULk_8cPF",
        "colab_type": "code",
        "outputId": "91777db9-505d-4aec-8a97-cc0fdb0b82f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "# ściągnięcie i rozpakowanie wektorów FastText\n",
        "!curl -o cc.pl.300.bin.gz https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.pl.300.bin.gz\n",
        "!gunzip cc.pl.300.bin.gz"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 4294M  100 4294M    0     0  38.9M      0  0:01:50  0:01:50 --:--:-- 26.4M\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ss2i4TrzBjRX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# parsowanie za pomocą UDPipe\n",
        "! wget https://github.com/ufal/udpipe/releases/download/v1.2.0/udpipe-1.2.0-bin.zip\n",
        "! unzip udpipe-1.2.0-bin.zip\n",
        "! rm udpipe-1.2.0-bin.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rw1iynunBoD-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ['PATH'] = os.environ['PATH'] + ':udpipe-1.2.0-bin/bin-linux64/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGYTmrWCBq7r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! wget http://mozart.ipipan.waw.pl/~alina/Polish_dependency_parsing_models/190423_PDBUD_ttp_embedd.udpipe"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYQiQhv_BtGt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! udpipe --tokenize --outfile=train_tokenised.conllu 190423_PDBUD_ttp_embedd.udpipe training_set_clean_only_text.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4TaUxfHBz-s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! udpipe --tag --parse --outfile=train_udpipe_parsed.conllu 190423_PDBUD_ttp_embedd.udpipe train_tokenised.conllu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ib9CNCtFB1bS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! udpipe --tokenize --outfile=test_tokenised.conllu 190423_PDBUD_ttp_embedd.udpipe test_set_clean_only_text.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkRobIvkB2sl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! udpipe --tag --parse --outfile=test_udpipe_parsed.conllu 190423_PDBUD_ttp_embedd.udpipe test_tokenised.conllu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnuYqJCnIY8M",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "6d8d6dcf-75a0-427a-e9f0-cd0cd06f9578"
      },
      "source": [
        "# załadowanie plików z danymi\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d1cb7bcb-7578-4374-96ee-fa6c4d421512\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-d1cb7bcb-7578-4374-96ee-fa6c4d421512\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving slownikWydzwieku01.csv to slownikWydzwieku01.csv\n",
            "Saving test_set_clean_only_tags.txt to test_set_clean_only_tags.txt\n",
            "Saving test_set_clean_only_text.txt to test_set_clean_only_text.txt\n",
            "Saving test_udpipe_parsed.conllu to test_udpipe_parsed.conllu\n",
            "Saving train_udpipe_parsed.conllu to train_udpipe_parsed.conllu\n",
            "Saving training_set_clean_only_tags.txt to training_set_clean_only_tags.txt\n",
            "Saving training_set_clean_only_text.txt to training_set_clean_only_text.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTMH-ejYmcE_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# wczytanie danych\n",
        "with open('training_set_clean_only_text.txt', 'r') as f:\n",
        "  train_text = f.readlines()\n",
        "with open('test_set_clean_only_text.txt', 'r') as f:\n",
        "  test_text = f.readlines()\n",
        "train_tags = np.loadtxt('training_set_clean_only_tags.txt', dtype=int)\n",
        "test_tags = np.loadtxt('test_set_clean_only_tags.txt', dtype=int)\n",
        "\n",
        "# wczytanie wyników parsowania\n",
        "with open('train_udpipe_parsed.conllu', 'r') as f:\n",
        "  train_parsed = f.readlines()\n",
        "with open('test_udpipe_parsed.conllu', 'r') as f:\n",
        "  test_parsed = f.readlines()\n",
        "  \n",
        "# wczytanie słownika wydźwięku\n",
        "sentiment = pd.read_csv(\"slownikWydzwieku01.csv\",sep=\"\\t\",header=None)\n",
        "sentimentDict = sentiment.set_index(0).T.to_dict('list')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbpPwr4jqawq",
        "colab_type": "code",
        "outputId": "e2c6f87c-39cd-4547-9085-90430ab77cea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# baseline - losowanie klasy na podstawie ich częstości w zbiorze uczącym\n",
        "freq0 = len(train_tags[train_tags==0])/len(train_tags)\n",
        "print('Odsetek próbek klasy 0:', freq0)\n",
        "out = ['0\\n' if np.random.uniform() < freq0 else '1\\n' for i in range(len(test_text))]\n",
        "with open('resultsBaseline.txt', 'w') as f:\n",
        "  f.writelines(out)\n",
        "files.download('resultsBaseline.txt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Odsetek próbek klasy 0: 0.915247485310228\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBsKDe2kwDQi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# wczytanie modelu wektorów FastText\n",
        "vecModel = FastText.load_fasttext_format('cc.pl.300')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qNRfjjlfJPt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#funkcja lematyzująca na podstawie banku drzew\n",
        "\n",
        "lemmas={}\n",
        "\n",
        "for el in train_parsed:\n",
        "  if len(el.split(\"\\t\"))>=3:\n",
        "    lemmas[el.split(\"\\t\")[1]]=el.split(\"\\t\")[2]\n",
        "for el in test_parsed:\n",
        "  if len(el.split(\"\\t\"))>=3:\n",
        "    lemmas[el.split(\"\\t\")[1]]=el.split(\"\\t\")[2]\n",
        "\n",
        "def lemma(w):\n",
        "  if lemmas[w]=='_':\n",
        "    if w[-2:] in ['em','eś']:\n",
        "      third_pers=w[:-2]\n",
        "    elif w[-1] in ['m','ś']:\n",
        "      third_pers=w[:-1]\n",
        "    elif w[-3:]=='śmy':\n",
        "      third_pers=w[:-3]\n",
        "    elif w[-4:]=='ście':\n",
        "      third_pers=w[:-3]\n",
        "    else:\n",
        "      third_pers=w\n",
        "    return lemmas[third_pers]\n",
        "  return lemmas[w]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgUWpkspBPgq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dla każdego przykładu zwraca, ile było słów spoza słownika (pomocnicza funkcja poglądowa niewykorzystywana w uczeniu modeli)\n",
        "def getMistakeCnts(text):\n",
        "  cntVec = np.zeros(len(text))\n",
        "  for i in range(len(text)):\n",
        "    #tokens = list(tokenize(text[i], lowercase = True)) \n",
        "    tokens = text_to_word_sequence(text[i], filters='!\"#$%&()*+,-;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
        "    cnt = 0\n",
        "    for token in tokens:\n",
        "      if token == 'anonymized_account' or re.search('http', token):\n",
        "        continue\n",
        "      if (token not in vecModel.wv.vocab):\n",
        "        cnt += 1\n",
        "    cntVec[i] = cnt\n",
        "  return cntVec\n",
        "  \n",
        "mistakesVec_train = getMistakeCnts(train_text)\n",
        "print(sum(mistakesVec_train != 0), '/', len(train_text), 'zdań z błędami na zbiorze uczącym')\n",
        "print(pearsonr(mistakesVec_train, train_tags))   # korelacja raczej nieduża\n",
        "mistakesVec_test = getMistakeCnts(test_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzzeIFLtZDDZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# zamiana zdań na wektory (średnia z wektorów słów)\n",
        "# lemmatize - zamienia słowa na lematy\n",
        "# addMistakesCnt - dodaje do wektorów liczbę słów w zdaniu, których nie było w słowniku\n",
        "# sentimentCols - kolumny ze słownika wydźwięku dodane do wektorów (kolumny z wydźwiękiem: 1-4)\n",
        "# mistakeMult, sentimentMult - mnożniki dla addMistakesCnt i sentimentCols, żeby bardziej się liczyły\n",
        "def text2Vectors(text, embeddingDim = 300, lemmatize = False, addMistakesCnt = False, sentimentCols = (2,), mistakeMult = 10, sentimentMult = 10):\n",
        "  vecLen = embeddingDim\n",
        "  if addMistakesCnt: vecLen += 1\n",
        "  vecLen += len(sentimentCols)\n",
        "  sentVecs = np.zeros((len(text), vecLen))\n",
        "  for i in range(len(text)):\n",
        "    #tokens = list(tokenize(text[i], lowercase = True))   # usuwa liczby i interpunkcję\n",
        "    tokens = text_to_word_sequence(text[i], filters='!\"#$%&()*+,-;<=>?@[\\\\]^_`{|}~\\t\\n')   # usuwa znaki z filters\n",
        "    embVec = np.zeros(embeddingDim)\n",
        "    sentimentVec = np.zeros(len(sentimentCols))\n",
        "    cnt = 0\n",
        "    misCnt = 0\n",
        "    for token in tokens:\n",
        "      if token == 'anonymized_account' or re.search('http', token):\n",
        "        continue\n",
        "      if (addMistakesCnt and token not in vecModel.wv.vocab):\n",
        "        misCnt += 1\n",
        "      try:\n",
        "        tokLemma = lemma(token)\n",
        "        if lemmatize: token = tokLemma\n",
        "        if sentimentCols: sentimentWord = [sentimentDict[tokLemma][col] for col in sentimentCols]\n",
        "      except KeyError:\n",
        "        if sentimentCols: sentimentWord = np.zeros(len(sentimentCols))\n",
        "      try:      \n",
        "        embVec += vecModel.wv[token]\n",
        "        if sentimentCols: sentimentVec += sentimentWord\n",
        "        cnt += 1\n",
        "      except KeyError:\n",
        "        continue\n",
        "    if not(cnt):\n",
        "      continue\n",
        "    vec = embVec / cnt \n",
        "    if addMistakesCnt: vec = np.append(vec, mistakeMult * misCnt)\n",
        "    if sentimentCols: vec = np.append(vec, sentimentMult * sentimentVec / cnt)\n",
        "    sentVecs[i,:] = vec\n",
        "  return sentVecs\n",
        "\n",
        "sentVecsTrain = text2Vectors(train_text)\n",
        "sentVecsTest = text2Vectors(test_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cutStIP-DdE9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# PCA na wektorach\n",
        "embeddingDim = 300\n",
        "newEmbeddingDim = 100\n",
        "pcaModel = PCA(n_components=newEmbeddingDim)\n",
        "newEmbTrain = pcaModel.fit_transform(sentVecsTrain[:,:embeddingDim])\n",
        "pcaSentVecsTrain = np.concatenate((newEmbTrain, sentVecsTrain[:,embeddingDim:]), axis=1)\n",
        "newEmbTest = pcaModel.transform(sentVecsTest[:,:embeddingDim])\n",
        "pcaSentVecsTest = np.concatenate((newEmbTest, sentVecsTest[:,embeddingDim:]), axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Xc6W3Z09gTU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# bag of words\n",
        "vectorizer = CountVectorizer()\n",
        "bags_train = vectorizer.fit_transform(train_text)\n",
        "bags_test = vectorizer.transform(test_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9L-V0P_D82L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# SVM na wektorach albo bags of wordsach\n",
        "svmModel = svm.SVC(class_weight = {0:0.1, 1:0.9}, C = 10000, gamma='auto')\n",
        "svmModel.fit(sentVecsTrain, train_tags)\n",
        "#svmModel.fit(pcaSentVecsTrain, train_tags)\n",
        "#svmModel.fit(bags_train, train_tags)\n",
        "preds = svmModel.predict(sentVecsTest)\n",
        "#preds = svmModel.predict(pcaSentVecsTest)\n",
        "#preds = svmModel.predict(bags_test)\n",
        "np.savetxt('resultsSVM.txt', preds, fmt='%d')\n",
        "files.download('resultsSVM.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3bRZNvi47M3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# czyszczenie tekstu z @anonymized_account i URL-i dla sieci LSTM\n",
        "train_text_clean = []\n",
        "for line in train_text:\n",
        "  train_text_clean.append(re.sub('@anonymized_account|http\\S+\\s', '', line))\n",
        "test_text_clean = []\n",
        "for line in test_text:\n",
        "  test_text_clean.append(re.sub('@anonymized_account|http\\S+\\s', '', line))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwDg3IHWo101",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# nadpróbkowanie przykładów z mową nienawiści (do sprawdzenia zamiast albo razem z parametrem class_weight przy uczeniu modelu)\n",
        "train_hate_text = np.repeat(np.array(train_text_clean)[train_tags==1], 9)\n",
        "train_text_long = np.concatenate((np.array(train_text_clean)[train_tags==0], train_hate_text))\n",
        "train_hate_tags = np.repeat(np.array(train_tags)[train_tags==1], 9)\n",
        "train_tags_long = np.concatenate((train_tags[train_tags==0], train_hate_tags))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJOG4E5dECW3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# lematyzacja całego tekstu\n",
        "def lemmatizeText(text):\n",
        "  text_lemma = []\n",
        "  for i in range(len(text)):\n",
        "    tokens = list(tokenize(text[i], lowercase = True))   # usuwa liczby i interpunkcję\n",
        "    #tokens = text_to_word_sequence(text[i], filters='!\"#$%&()*+,-;<=>?@[\\\\]^_`{|}~\\t\\n')   # usuwa znaki z filters\n",
        "    line = []\n",
        "    for token in tokens:\n",
        "      if token == 'anonymized_account' or re.search('http', token):  \n",
        "        continue\n",
        "      try:\n",
        "        line.append(lemma(token))\n",
        "      except KeyError:\n",
        "        line.append(token)\n",
        "    text_lemma.append(' '.join(line))\n",
        "  return text_lemma\n",
        "\n",
        "train_text_lemmas = lemmatizeText(train_text)\n",
        "test_text_lemmas = lemmatizeText(test_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qdvSjrfBVB7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sieć LSTM, która sama będzie uczyć się wektorów słów\n",
        "\n",
        "# tokenizacja i utworzenie sekwencji indeksów tokenów\n",
        "maxLen = 40   # maksymalna długość zdania (musi być stała, bo tak chce warstwa Embedding)\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_text_clean)\n",
        "#tokenizer.fit_on_texts(train_text_lemmas)\n",
        "train_seqs = tokenizer.texts_to_sequences(train_text_clean)\n",
        "#train_seqs = tokenizer.texts_to_sequences(train_text_lemmas)\n",
        "\n",
        "train_seqs = pad_sequences(train_seqs, padding='post', maxlen=maxLen)\n",
        "\n",
        "# utworzenie i nauka modelu\n",
        "embeddingDim = 100   # długość wektorów\n",
        "lstmModel = Sequential()\n",
        "lstmModel.add(Embedding(len(tokenizer.word_index) + 1, embeddingDim, input_length=maxLen))\n",
        "#lstmModel.add(LSTM(embeddingDim, unroll=True))\n",
        "lstmModel.add(Bidirectional(LSTM(embeddingDim, unroll=True)))\n",
        "#lstmModel.add(SimpleRNN(embeddingDim, unroll=True))\n",
        "#lstmModel.add(GRU(embeddingDim, unroll=True))\n",
        "#lstmModel.add(Dropout(0.4)) \n",
        "lstmModel.add(Dense(1, activation='sigmoid'))\n",
        "lstmModel.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "lstmModel.summary()\n",
        "history = lstmModel.fit(train_seqs, train_tags, epochs=10, class_weight = {0:0.1, 1:0.9})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYRlkmccMwCb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# zwraca macierz wektorów używaną przez sieci LSTM\n",
        "# addMistakes - dodaje do wektorów 1, jeśli słowa nie było w słowniku i 0, jeśli było\n",
        "# sentimentCols - kolumny ze słownika wydźwięku dodane do wektorów (kolumny z wydźwiękiem: 1-4)\n",
        "# mistakeMult, sentimentMult - mnożniki dla addMistakes i sentimentCols, żeby bardziej się liczyły\n",
        "def getVectorMatrix(tokenizer, embeddingDim = 300, addMistakes = False, sentimentCols = (), mistakeMult = 1, sentimentMult = 100):\n",
        "  vecLen = embeddingDim\n",
        "  if addMistakes: vecLen += 1\n",
        "  vecLen += len(sentimentCols)\n",
        "  vecMatrix = np.zeros((len(tokenizer.word_index) + 1, vecLen))\n",
        "  for word, index in tokenizer.word_index.items():  \n",
        "    try:\n",
        "      wordLemma = lemma(word)\n",
        "      if sentimentCols: sentimentVec = np.array([sentimentDict[wordLemma][col] for col in sentimentCols])\n",
        "    except KeyError:\n",
        "      if sentimentCols: sentimentVec = np.zeros(len(sentimentCols))      \n",
        "    try:    \n",
        "      vector = vecModel.wv[word]\n",
        "      if addMistakes: \n",
        "        mistake = 0 if word in vecModel.wv.vocab else 1\n",
        "        vector = np.append(vector, mistake * mistakeMult)\n",
        "      if sentimentCols: vector = np.append(vector, sentimentVec * sentimentMult)\n",
        "      vecMatrix[index] = vector\n",
        "    except KeyError:\n",
        "      continue\n",
        "  return vecMatrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSXqolVV_Ql8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sieć LSTM z wyuczonymi wektorami słów FastText (warstwa Embedding z narzuconymi wagami niepodlegającymi uczeniu)\n",
        "\n",
        "# tokenizacja i utworzenie sekwencji indeksów tokenów\n",
        "maxLen = 40   # maksymalna długość zdania\n",
        "embeddingDim = 300   # długość wektorów (taka jest w FastText)\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_text_clean)\n",
        "train_seqs = tokenizer.texts_to_sequences(train_text_clean)\n",
        "train_seqs = pad_sequences(train_seqs, padding='post', maxlen=maxLen)\n",
        "\n",
        "# macierz wektorów słów\n",
        "vecMatrix = getVectorMatrix(tokenizer)\n",
        "\n",
        "# PCA\n",
        "newEmbeddingDim = 100  \n",
        "pcaModel = PCA(n_components=newEmbeddingDim)\n",
        "newEmbTrain = pcaModel.fit_transform(vecMatrix[:,:embeddingDim])\n",
        "pcaVecMatrix = np.concatenate((newEmbTrain, vecMatrix[:,embeddingDim:]), axis=1)\n",
        "\n",
        "# utworzenie i nauka modelu\n",
        "lstmModel = Sequential()\n",
        "lstmModel.add(Embedding(len(tokenizer.word_index) + 1, pcaVecMatrix.shape[1], input_length=maxLen, weights=[pcaVecMatrix], trainable=False))\n",
        "#lstmModel.add(LSTM(newEmbeddingDim))\n",
        "lstmModel.add(Bidirectional(LSTM(newEmbeddingDim, unroll=True)))\n",
        "#lstmModel.add(Dropout(0.5))\n",
        "lstmModel.add(Dense(1, activation='sigmoid'))\n",
        "lstmModel.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "lstmModel.summary()\n",
        "history = lstmModel.fit(train_seqs, train_tags, epochs=10, class_weight = {0:0.1, 1:0.9})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaBehaTiAuLM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# testowanie powyższych dwóch sieci (nowe słowa, dla których nie ma wektorów, są odrzucane)\n",
        "test_seqs = tokenizer.texts_to_sequences(test_text_clean)\n",
        "test_seqs = pad_sequences(test_seqs, padding='post', maxlen=maxLen)\n",
        "preds = lstmModel.predict(test_seqs)\n",
        "preds[preds > 0.5] = 1\n",
        "preds[preds <= 0.5] = 0\n",
        "np.savetxt('resultsLSTM.txt', preds, fmt='%d')\n",
        "files.download('resultsLSTM.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1Iw7-D8Nfq7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# źródło: https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric\n",
        "# liczenie miary F1 i błędu F1 do uczenia modelu\n",
        "\n",
        "from keras import backend as K\n",
        "import tensorflow as tf\n",
        "\n",
        "def f1(y_true, y_pred):\n",
        "  y_pred = K.round(y_pred)\n",
        "  tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
        "  tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
        "  fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
        "  fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
        "\n",
        "  p = tp / (tp + fp + K.epsilon())\n",
        "  r = tp / (tp + fn + K.epsilon())\n",
        "\n",
        "  f1 = 2*p*r / (p+r+K.epsilon())\n",
        "  f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
        "  return K.mean(f1)\n",
        "\n",
        "def f1_loss(y_true, y_pred):  \n",
        "  tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
        "  tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
        "  fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
        "  fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
        "\n",
        "  p = tp / (tp + fp + K.epsilon())\n",
        "  r = tp / (tp + fn + K.epsilon())\n",
        "\n",
        "  f1 = 2*p*r / (p+r+K.epsilon())\n",
        "  f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
        "  return 1 - K.mean(f1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkXBgEQWmNig",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sieć LSTM na wektorach FastText (bez warstwy Embedding)\n",
        "\n",
        "# tokenizacja i utworzenie sekwencji indeksów tokenów\n",
        "embeddingDim = 300   # długość wektorów (taka jest w FastText)\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_text_clean)\n",
        "train_seqs = tokenizer.texts_to_sequences(train_text_clean)\n",
        "maxLen = 40\n",
        "train_seqs = pad_sequences(train_seqs, padding='post', maxlen=maxLen)\n",
        "\n",
        "# macierz wektorów słów\n",
        "vecMatrix = getVectorMatrix(tokenizer)\n",
        "print(vecMatrix.shape)\n",
        "\n",
        "# PCA\n",
        "#newEmbeddingDim = 50  \n",
        "#pcaModel = PCA(n_components=newEmbeddingDim)\n",
        "#newEmbTrain = pcaModel.fit_transform(vecMatrix[:,:embeddingDim])\n",
        "#pcaVecMatrix = np.concatenate((newEmbTrain, vecMatrix[:,embeddingDim:]), axis=1)\n",
        "\n",
        "# zamiana indeksów na wektory (to, co robi warstwa Embedding)\n",
        "train_seqs = np.array([vecMatrix[seq,] for seq in train_seqs])\n",
        "#train_seqs = np.array([pcaVecMatrix[seq,] for seq in train_seqs])\n",
        "\n",
        "# utworzenie i nauka modelu\n",
        "lstmModel = Sequential()\n",
        "#lstmModel.add(LSTM(newEmbeddingDim))\n",
        "#lstmModel.add(Bidirectional(LSTM(100, return_sequences=True)))\n",
        "lstmModel.add(Bidirectional(LSTM(20)))\n",
        "#lstmModel.add(Dropout(0.4))\n",
        "lstmModel.add(Dense(1, activation='sigmoid'))\n",
        "lstmModel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', f1], validation_data=(test_seqs, test_tags))\n",
        "#lstmModel.summary()\n",
        "history = lstmModel.fit(train_seqs, train_tags, epochs=10, class_weight = {0:0.1, 1:0.9}, batch_size = 200)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0YqPplTtap2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# testowanie powyższej sieci (nowe słowa dostają wektory z FastText)\n",
        "\n",
        "# tokenizacja i utworzenie sekwencji indeksów tokenów\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(test_text_clean)\n",
        "test_seqs = tokenizer.texts_to_sequences(test_text_clean)\n",
        "test_seqs = pad_sequences(test_seqs, padding='post', maxlen=maxLen)\n",
        "\n",
        "# macierz wektorów słów\n",
        "vecMatrix = getVectorMatrix(tokenizer)\n",
        "\n",
        "# PCA\n",
        "#newEmbTest = pcaModel.transform(vecMatrix[:,:embeddingDim])\n",
        "#pcaVecMatrix = np.concatenate((newEmbTest, vecMatrix[:,embeddingDim:]), axis=1)\n",
        "\n",
        "# zamiana indeksów na wektory (to, co robi warstwa Embedding)\n",
        "test_seqs = np.array([vecMatrix[seq,] for seq in test_seqs])\n",
        "#test_seqs = np.array([pcaVecMatrix[seq,] for seq in test_seqs])\n",
        "\n",
        "# predykcja\n",
        "preds = lstmModel.predict(test_seqs)\n",
        "preds[preds > 0.5] = 1\n",
        "preds[preds <= 0.5] = 0\n",
        "np.savetxt('resultsLSTM.txt', preds, fmt='%d')\n",
        "files.download('resultsLSTM.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7EnX8ZvIgxs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lstmModel.evaluate(test_seqs, test_tags, batch_size=len(test_seqs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMb45IMRLRjD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# usuwa słowa występujące rzadziej niż count_thres (w końcu to nie jest używane)\n",
        "def preprocess(text, count_thres):\n",
        "  tokenizer = Tokenizer(filters='!\"#$%&()*+,-;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
        "  tokenizer.fit_on_texts(text)\n",
        "  words_to_remove = ['anonymized', 'account']\n",
        "  words_to_remove.extend([w for w,c in tokenizer.word_counts.items() if c < count_thres])\n",
        "  before_cnt = len(tokenizer.word_index)\n",
        "  for w in words_to_remove:\n",
        "    del tokenizer.word_index[w]\n",
        "    del tokenizer.word_docs[w]\n",
        "    del tokenizer.word_counts[w]\n",
        "  text_seqs = tokenizer.texts_to_sequences(train_text)\n",
        "  return (tokenizer, text_seqs, before_cnt)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}